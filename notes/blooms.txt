#----------------------------------------------------------------------------------------------------------------------------
### conda env for downloading CMEMS data. 

# following the instructions on https://help.marine.copernicus.eu/en/articles/7970514-copernicus-marine-toolbox-installation

# From loggin node home directory.
module load lang/Anaconda3/2023.03-1

# Then conenct to the sandbox compute node. 
srun -I30 -p sandbox -N 1 -c 1 --mem=6G -t 0-01:00:00 --pty /bin/bash

#------------------------------------------------------------------------------
### CMEMS toolbox env. setup and testing.

# I'm following the directions to install cmems toolbox so
# that I can download their data using the command line. 
# First I create the .yml file 
jamie@jamie:~/projects/blooms/data$ vim copernicus-marine-client-env.yml

# Then I create a conda env using that .yml file. Then activate.  
jamie@jamie:~/projects/blooms/data$ conda env create --file copernicus-marine-client-env.yml
jamie@jamie:~/projects/blooms/data$ conda activate cmc-beta

# This gives information about the env. 
(cmc-beta) jamie@jamie:~/projects/blooms/data$ conda env list && copernicus-marine --help

# To update the env. first delete the hidden file then run...
rm .copernicus-marine-client 
python -m pip install copernicus-marine-client --upgrade & copernicus-marine describe --overwrite-metadata-cache > catalogue.json
# Check teh version and confirm it worked.  
(cmc-beta) jamie@jamie:~$ copernicus-marine --version
copernicus-marine, version 0.10.3

# output available datasets. 
copernicus-marine describe
# to save the output as a .json file. 
(cmc-beta) jamie@jamie:~/projects/blooms/data$ copernicus-marine subset --dataset-id cmems_mod_glo_phy-cur_anfc_0.083deg_P1D-m --start-datetime 2022-01-01 --end-datetime 2022-01-15 --minimum-longitude 5.0 --maximum-longitude 10.0 --minimum-latitude 38.0 --maximum-latitude 42.0
user: jash
password: 5.Pellegrino

# cool the test download seems to work. 
# now I will do this on the HPC

# conda env
cmc-beta

#----------------------------------------------------------------------------------------------------------------------------
# Local CHL download test.
# dataset id: c3s_obs-oc_glo_bgc-plankton_my_l3-multi-4km_P1D
# username: jash
# password: 5.Pellegrino

# Downloading a small data set. Using template from CMEMS
copernicus-marine subset -i c3s_obs-oc_glo_bgc-plankton_my_l3-multi-4km_P1D -y 18.0 -Y 24.0 -x -170.0 -X -155.0 -v CHL -t 2022-01-01 -T 2022-01-15 -o ./copernicus-data -f chl_test.nc

# so that seems to work.
# I should download a small data set and load it up on a local instance of R to test the download.

# Test donloading one month of data.
(cmc-beta) jamie@jamie:~/projects/blooms/data$ copernicus-marine subset -i c3s_obs-oc_glo_bgc-plankton_my_l3-multi-4km_P1D -y 16.0 -Y 40.0 -x -175.0 -X -130.0 -v CHL -t 2022-01-01 -T 2022-02-06 -o . -f chl_test_size.nc
(cmc-beta) jamie@jamie:~/projects/blooms/data$ ls -lh
# We're looking at about 18-20gb of data for the entire thing.
-rw-rw-r-- 1 jamie jamie 88M Dec 24 10:03 chl_test_size.nc

#----------------------------------------------------------------------------------------------------------------------------
### Local CHL download large daily data.
# dataset id: c3s_obs-oc_glo_bgc-plankton_my_l3-multi-4km_P1D

# YOLO the big dady dataset. 
(cmc-beta) jamie@jamie:~/projects/blooms/data$ copernicus-marine subset -i c3s_obs-oc_glo_bgc-plankton_my_l3-multi-4km_P1D -y 16.0 -Y 40.0 -x -175.0 -X -130.0 -v CHL -t 1998-01-01 -T 2023-12-30 -o . -f chl_1998_2023_l3_multi_4k.nc

# heads up to future self. The time range was set into the future so it may not have worked. 

# I've started teh data transfer to the HPC.
scp chl_1998_2023_l3_multi_4k.nc jamesash@koa.its.hawaii.edu:/home/jamesash/blooms/data/

#----------------------------------------------------------------------------------------------------------------------------
### Local CHL download monthly data.
# dataset id: c3s_obs-oc_glo_bgc-plankton_my_l3-multi-4km_P1D

# YOLO the big dady dataset. 
(cmc-beta) jamie@jamie:~/projects/blooms/data$ copernicus-marine subset -i cmems_obs-oc_glo_bgc-plankton_my_l4-multi-4km_P1M -y 16.0 -Y 40.0 -x -175.0 -X -130.0 -v CHL -t 1998-01-01 -T 2023-12-30 -o . -f chl_1998_2023_l4_month_multi_4k.nc

# heads up to future self. The time range was set into the future so it may not have worked. 

# Transfer to the HPC.
scp chl_1998_2023_l4_month_multi_4k.nc jamesash@koa.its.hawaii.edu:/home/jamesash/blooms/data/


#----------------------------------------------------------------------------------------------------------------------------
### Local CHL download small daily data.
# dataset id: c3s_obs-oc_glo_bgc-plankton_my_l3-multi-4km_P1D

(cmc-beta) jamie@jamie:~/projects/blooms/data$ copernicus-marine subset -i c3s_obs-oc_glo_bgc-plankton_my_l3-multi-4km_P1D -y 16.0 -Y 40.0 -x -175.0 -X -130.0 -v CHL -t 2022-01-01 -T 2023-11-01 -o . -f chl_2022_2023_l3_daily_multi_4k.nc

# Transfer to the HPC.
scp chl_2022_2023_l3_daily_multi_4k.nc jamesash@koa.its.hawaii.edu:/home/jamesash/blooms/data/

#----------------------------------------------------------------------------------------------------------------------------
### Create a script that makes a figure for the small test data set. 

# From loggin node home directory.
module load lang/Anaconda3/2023.03-1

# Then conenct to the sandbox compute node.
# I needed more power for it to run.
srun -I30 -p sandbox -N 1 -c 2 --mem=50G -t 0-01:00:00 --pty /bin/bash

# I initiate a R session.
(blooms) [jamesash@cn-03-13-02 code]$ R

# I donloaded a package this way.
> install.packages("anytime")

# Then I run stepwise the contents of testmap.R.

# scp the figure home. 

#----------------------------------------------------------------------------------------------------------------------------
### Setting up blooms env. 

module load lang/Anaconda3/2023.03-1

# Then conenct to the sandbox compute node. 
srun -I30 -p sandbox -N 1 -c 4 --mem=60G -t 0-03:00:00 --pty /bin/bash

# I creat a new codna env called "blooms" with an up-to-date python. 
conda create --name blooms r-essentials r-base

# activate the env. and install libraries 
[jamesash@cn-03-13-01 code]$ source activate blooms
(blooms) [jamesash@cn-03-13-01 code]$ conda install r-raster
(blooms) [jamesash@cn-03-13-01 code]$ conda install r-ncdf4

(blooms) [jamesash@cn-03-13-01 code]$ conda install r-
(blooms) [jamesash@cn-03-13-01 code]$ conda install r-

# To install packages outside of the r-essentials and CRAN I need to 
# use an R instance.
(blooms) [jamesash@cn-03-13-01 code]$ R
> install.packages("devtools")
> install.packages("oce")
> install.packages("cmocean")
> install.packages("lubridate")
> install.packages("terra")

#----------------------------------------------------------------------------------------------------------------------------
### Run individual jobs on the shared partition

# I use a slurm script with the following header

[jamesash@login-0101 code]$ less anomaly_plot.slurm 

#!/bin/bash
#SBATCH --job-name=cli_map

#SBATCH --ntasks=1

#SBATCH --partition=shared
## 3 day max run time for public partitions, except 4 hour limit  in sandbox
#SBATCH --time=0-02:30:00 ## time format is DD-HH:MM:SS

#SBATCH --cpus-per-task=4
#SBATCH --mem=300G ## max amount of memory per node you require

#SBATCH --error=hello-%A_%a.err ## %A - filled with jobid
#SBATCH --output=hello-%A_%a.out ## %A - filled with jobid

##SBATCH --mail-type=BEGIN,END,FAIL,REQUEUE,TIME_LIMIT_80
##SBATCH --mail-user=user@test.org
##SBATCH --array=1-10
##SBATCH --core-spec=0 ## Uncomment to allow jobs to request all cores on a node    

## All options and environment variables found on schedMD site: http://slurm.schedmd.com/sbatch.html

## module purge
module load lang/Anaconda3/2023.03-1
 
source activate blooms

Rscript ./chl_loc_reg.R

#----------------------------------------------------------------------------------------------------------------------------
### Seting up github. 

[jamesash@login-0101 ~]$ module load lang/Anaconda3/2023.03-1
[jamesash@login-0101 ~]$ module load tools/git/2.38.1-GCCcore-12.2.0-nodocs

# Then conenct to the sandbox compute node.
[jamesash@login-0101 ~]$ srun -I30 -p sandbox -N 1 -c 4 --mem=60G -t 0-03:00:00 --pty /bin/bash

# writing this from memory so may be inacurate. 
[jamesash@cn-03-13-04 code]$ git init
[jamesash@cn-03-13-04 code]$ git add *.R 
[jamesash@cn-03-13-04 code]$ git commit -m "Initial commit"
[jamesash@cn-03-13-04 code]$ git config --global user.name "jamiesash"
[jamesash@cn-03-13-04 code]$ git config --global user.email "jamesash@hawaii.edu"

# create a repository on my git hub page via the internet. Call it climate_maps_hpc
[jamesash@cn-03-13-04 code]$ git remote add origin git@github.com:jamiesash/climate_maps_hpc.git
[jamesash@cn-03-13-04 code]$ git remote add origin jamesash@hawaii.edu:jamiesash/climate_maps_hpc.git
[jamesash@cn-03-13-04 code]$ git push -u origin main
# And I get a warnning that files arent pushed to the main....

# Personal access token. 

# starting over from a new directory climate
[jamesash@cn-03-13-04 climate]$ git init
[jamesash@cn-03-13-04 climate]$ git remote add origin jamesash@hawaii.edu:jamiesash/climate_maps.git
[jamesash@cn-03-13-04 climate]$ mkdir code
[jamesash@cn-03-13-04 climate]$ cp ../blooms/code/* ./code/
[jamesash@cn-03-13-04 climate]$ git add .
[jamesash@cn-03-13-04 climate]$ git commit -m "Initial commit"
[jamesash@cn-03-13-04 climate]$ git push -u origin main
# timeout failure
[jamesash@cn-03-13-04 climate]$ git config --global user.name "jamiesash"
[jamesash@cn-03-13-04 climate]$ git config --global user.email "jamesash@hawii.edu"
[jamesash@cn-03-13-04 climate]$ git push --set-upstream origin main 
# timedout failure
# At this point I'm going to say fug it. 
# my project is being tracke using git in climate. 
# at some point I may figure out how to push this to github.
# mayb I'll try cloning a project localy. 

# ------------------------------------------------------------------------------------------------------------------------------------
# Downloading SLA from cmems

# Downloading the daily ssh.
# product id is SEALEVEL_GLO_PHY_L4_MY_008_047

# actate conda env first of course then run copernicous
(cmc-beta) jamie@jamie:~/projects/climate/data/sla$ conda activate 
(cmc-beta) jamie@jamie:~/projects/climate/data/sla$ copernicus-marine subset -i cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D -y 16.0 -Y 40.0 -x -175.0 -X -130.0 -v sla -t 1998-01-01 -T 2023-12-30 -o . -f sla_1998_2023_l4_4k.nc

# Not too large of a file.
-rw-rw-r-- 1 jamie jamie 613M Feb 22 10:26 sla_1998_2023_l4_4k.nc


# ------------------------------------------------------------------------------------------------------------------------------------
# Downloading FSLE from AVISO

copernicus-marine subset -i cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D -y 16.0 -Y 40.0 -x -175.0 -X -130.0 -v sla -t 1998-01-01 -T 2023-12-30 -o . -f sla_1998_2023_l4_4k.nc
https://motu.aviso.altimetry.fr/motu-web/Motu?action=listcatalog&service=AvisoFSLE

# Just fsle max 2015
python motuclient.exe -u jamesash@hawaii.edu -p "liw3x9" -m https://motu.aviso.altimetry.fr/motu-web/Motu -s AvisoFSLE -id dataset-duacs-dt-global-allsat-madt-fsle -x -180 -X -120 -y 15 -Y 45 -t "2010-01-01" -T "2023-01-01" --outputWritten netcdf -v fsle_max -o C:\Users\james\Desktop\jamieslife\data\infiles\case_study -f fsle_po_2010_2023.nc

# directly from aviso
python motuclient.py -u jamesash@hawaii.edu -p "liw3x9" -m https://motu.aviso.altimetry.fr/motu-web/Motu -s AvisoFSLE -d dataset-duacs-dt-global-allsat-madt-fsle -x 0.019999999552965164 -X -0.019989013671875 -y -89.9800033569336 -Y 89.9800033569336 -t "2023-06-07" -T "2023-06-07" --outputWritten netcdf -v fsle_max -o your_output_directory(1) -f your_output_file_name(1) --proxy-server=your_proxy_server_url:your_proxy_port_number(2) --proxy-user=your_proxy_user_login(3) --proxy-pwd=your_proxy_user_password(3)

# add my info
python motuclient.py -u jamesash@hawaii.edu -p "liw3x9" -m https://motu.aviso.altimetry.fr/motu-web/Motu -s AvisoFSLE -d dataset-duacs-dt-global-allsat-madt-fsle -x -175 -X -130 -y 16 -Y 40 -t "1998-01-01" -T "2023-06-07" --outputWritten netcdf -v fsle_max -o . -f fsle_max_2010_2023.nc 

# idk it's fucked.
motuclient -u jamesash@hawaii.edu -p "liw3x9" -m https://motu.aviso.altimetry.fr/motu-web/Motu -s AvisoFSLE -d dataset-duacs-dt-global-allsat-madt-fsle -x -175 -X -130 -y 16 -Y 40 -t "1998-01-01" -T "2023-06-07" --outputWritten netcdf -v fsle_max -o . -f fsle_max_2010_2023.nc 

# im so tuired of motuclient
python /home/jamie/anaconda3/envs/aviso/lib/python3.12/site-packages/motuclient/motuclient.py -u jamesash@hawaii.edu -p "liw3x9" -m https://motu.aviso.altimetry.fr/motu-web/Motu -s AvisoFSLE -d dataset-duacs-dt-global-allsat-madt-fsle -x -130 -X -175 -y 16 -Y 40 -t "2023-01-01" -T "2023-02-01" --outputWritten netcdf -v fsle_max -o /home/jamie/projects/climate/data/fsle/ -f fsle_max_2023_test.nc

python /home/jamie/anaconda3/envs/aviso/lib/python3.12/site-packages/motuclient/motuclient.py -u jamesash@hawaii.edu -p "liw3x9" -m https://motu.aviso.altimetry.fr/motu-web/Motu -s AvisoFSLE -d dataset-duacs-dt-global-allsat-madt-fsle -x 110 -X -100 -y 0 -Y 70 -t "2023-05-07" -T "2023-06-07" --outputWritten netcdf -v fsle_max -o . -f fsle_test.nc

# ------------------------------------------------------------------------------------------------------------------------------------
# Downloading Sattelite data from CMEMS. 

# Test sea surface temperature. 
copernicusmarine subset -i METOFFICE-GLO-SST-L4-NRT-OBS-SST-V2 -y 18.0 -Y 24.0 -x -170.0 -X -155.0 -t 2022-01-01 -T 2022-01-15 -f tmp_test.nc -v analysed_sst

# To download temperature for all year. Might need mroe to calculate an anomaly. 
copernicusmarine subset -i METOFFICE-GLO-SST-L4-NRT-OBS-SST-V2 -y 16.0 -Y 40.0 -x -175.0 -X -130.0 -v analysed_sst -t 2007-01-01 -T 2022-12-30 -o . -f ssta_2018_2022_l4_4k.nc

# Downlaod a subset. Beta is slightly different.  
copernicusmarine subset -i METOFFICE-GLO-SST-L4-NRT-OBS-SST-V2 -y 16.0 -Y 40.0 -x -175.0 -X -130.0 -v analysed_sst -t 2018-01-01 -T 2018-12-30 -o . -f ssta_2018_l4_4k.nc
copernicusmarine subset -i cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D -y 16.0 -Y 40.0 -x -175.0 -X -130.0 -v sla -t 2018-01-01 -T 2018-12-30 -o . -f sla_2018_l4_4k.nc

# Downloading jsut the summer for the case study. 
copernicusmarine subset -i METOFFICE-GLO-SST-L4-NRT-OBS-SST-V2 -y 16.0 -Y 40.0 -x -175.0 -X -130.0 -v analysed_sst -t 2018-06-01 -T 2018-11-01 -o . -f ssta_2018_l4_4k.nc
copernicusmarine subset -i cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D -y 16.0 -Y 40.0 -x -175.0 -X -130.0 -v sla -t 2018-06-01 -T 2018-11-01 -o . -f sla_2018_l4_4k.nc
copernicusmarine subset -i c3s_obs-oc_glo_bgc-plankton_my_l3-multi-4km_P1D -y 16.0 -Y 40.0 -x -175.0 -X -130.0 -v CHL -t 2018-06-01 -T 2018-11-01 -o . -f chl_2018_daily_multi_l3_4k.nc

# To download the same timespan for SLA. Or just use the whole data set it is already an anomaly.  
copernicusmarine subset -i cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D -y 16.0 -Y 40.0 -x -175.0 -X -130.0 -v sla -t 2007-01-01 -T 2022-12-30 -o . -f sla_2018_2022_l4_4k.nc

# To download the same timespan for CHL. 

# ------------------------------------------------------------------------------------------------------------------------------------
# Actual data sets used for analysis 

# Downloading jut the summer for the case study.
# SST
copernicusmarine subset -i METOFFICE-GLO-SST-L4-NRT-OBS-SST-V2 -y 16.0 -Y 40.0 -x -175.0 -X -130.0 -v analysed_sst -t 2018-06-01 -T 2018-11-01 -o . -f sst_2018_l4_4k.nc
# SLA
copernicusmarine subset -i cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D -y 16.0 -Y 40.0 -x -175.0 -X -130.0 -v sla -t 2018-06-01 -T 2018-11-01 -o . -f sla_2018_l4_4k.nc
# CHL
copernicusmarine subset -i c3s_obs-oc_glo_bgc-plankton_my_l3-multi-4km_P1D -y 16.0 -Y 40.0 -x -175.0 -X -130.0 -v CHL -t 2018-06-01 -T 2018-11-01 -o . -f chl_2018_daily_multi_l3_4k.nc
# GlobColor L3
copernicusmarine subset -i cmems_obs-oc_glo_bgc-plankton_my_l3-multi-4km_P1D -y 16.0 -Y 40.0 -x -175.0 -X -130.0 -v CHL -t 2018-06-01 -T 2018-11-01 -o . -f chl_2018_daily_multi_l3_4k.nc
# GlobColor L4
copernicusmarine subset -i cmems_obs-oc_glo_bgc-plankton_my_l4-gapfree-multi-4km_P1D -y 16.0 -Y 40.0 -x -175.0 -X -130.0 -v CHL -t 2018-06-01 -T 2018-11-01 -o . -f chl_2018_glob_daily_multi_l4_4k.nc
# GlobColor Monthly
copernicusmarine subset -i  cmems_obs-oc_glo_bgc-plankton_my_l4-multi-4km_P1M-y 16.0 -Y 40.0 -x -175.0 -X -130.0 -v CHL -t 2018-06-01 -T 2018-11-01 -o . -f chl_2018_glob_monthly_multi_l4_4k.nc



# To download all of SLA.
copernicusmarine subset -i cmems_obs-sl_glo_phy-ssh_my_allsat-l4-duacs-0.25deg_P1D -y 16.0 -Y 40.0 -x -175.0 -X -130.0 -v sla -t 2007-01-01 -T 2022-12-30 -o . -f sla_2018_2022_l4_4k.nc

# To download all of CHL. 
copernicus-marine subset -i cmems_obs-oc_glo_bgc-plankton_my_l4-multi-4km_P1M -y 16.0 -Y 40.0 -x -175.0 -X -130.0 -v CHL -t 1998-01-01 -T 2023-12-30 -o . -f chl_1998_2023_l4_month_multi_4k.nc

# Downloading GlobColor for 2018
OCEANCOLOUR_GLO_BGC_L3_MY_009_103 

copernicus-marine subset -i cmems_obs-oc_glo_bgc-plankton_my_l4-multi-4km_P1M -y 16.0 -Y 40.0 -x -175.0 -X -130.0 -v CHL -t 1998-01-01 -T 2023-12-30 -o . -f chl_1998_2023_l4_month_multi_4k.nc

copernicusmarine subset -i cmems_obs-oc_glo_bgc-plankton_my_l3-multi-4km_P1D -y 16.0 -Y 40.0 -x -175.0 -X -130.0 -v CHL -t 2018-06-01 -T 2018-11-01 -o . -f chl_2018_daily_multi_l3_4k.nc























